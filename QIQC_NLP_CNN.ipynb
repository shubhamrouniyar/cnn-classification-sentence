{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embeddings', 'train.csv', 'sample_submission.csv', 'test.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  ...   target\n",
       "0  00002165364db923c7e6  ...        0\n",
       "1  000032939017120e6e44  ...        0\n",
       "2  0000412ca6e4628ce2cf  ...        0\n",
       "3  000042bf85aa498cd78e  ...        0\n",
       "4  0000455dfa3e01eae3af  ...        0\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../input/train.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "c5489b723770bbd7e5ccc0110183bbacbcec04dd"
   },
   "outputs": [],
   "source": [
    "def make_vocab(sentences):\n",
    "    '''Build a vocabulary based on the available text data. Returns a dictionary with word and its frequency.'''\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "2849c2124ac4e8204876b4d8ea76b381890edff8"
   },
   "outputs": [],
   "source": [
    "sentences = train_data['question_text'].apply(lambda x: x.split()).values\n",
    "vocab = make_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "040bffc1542beb5b9180ea710be65721af850b79"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format('../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin',binary=True,limit=600000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "6a9fd23c239455c01c0936be0ff8b40f9943f8a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('liverpool', 0.8414958715438843),\n",
       " ('man_utd', 0.8178720474243164),\n",
       " ('wenger', 0.812255322933197),\n",
       " ('real_madrid', 0.7843406200408936),\n",
       " ('barca', 0.7839786410331726),\n",
       " ('utd', 0.7657824754714966),\n",
       " ('everton', 0.7584188580513),\n",
       " ('drogba', 0.7512555122375488),\n",
       " ('madrid', 0.7511358261108398),\n",
       " ('fergie', 0.7509583830833435)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating how word2vec works.\n",
    "embeddings.most_similar('chelsea')\n",
    "# Since this embedding was derived using Google news data, there's a large correlation between chelsea and wenger!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "40badfa9adfe6b576248ec36e0ba898b438ea0db"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "def present_in_word2vec(vocab,embeddings):\n",
    "    '''Finds the intersection between embedding and our vocab.'''\n",
    "    in_embedding = {}\n",
    "    not_in_embedding = {}\n",
    "    in_embedding_word_count = 0\n",
    "    not_in_embedding_word_count = 0\n",
    "    for word in vocab:\n",
    "        try:\n",
    "            in_embedding[word] = embeddings[word]\n",
    "            in_embedding_word_count += vocab[word]\n",
    "        except:\n",
    "            not_in_embedding[word] = vocab[word]\n",
    "            not_in_embedding_word_count += vocab[word]\n",
    "            pass\n",
    "    \n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(in_embedding)/len(vocab)))\n",
    "    print('Found embeddings for {:.2%} of all text'.format(in_embedding_word_count/(in_embedding_word_count+not_in_embedding_word_count)))\n",
    "    x = sorted(not_in_embedding.items(),key=operator.itemgetter(1))[::-1]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "79afdb485bff49ebfa82593a235a5246e16f7477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 19.76% of vocab\n",
      "Found embeddings for 78.24% of all text\n"
     ]
    }
   ],
   "source": [
    "out_of_embed = present_in_word2vec(vocab,embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "122a78adbf1826fc96889babcf7e31097683f47c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 403183),\n",
       " ('a', 402682),\n",
       " ('of', 330825),\n",
       " ('and', 251973),\n",
       " ('India?', 16384),\n",
       " ('it?', 12900),\n",
       " (\"What's\", 12425),\n",
       " ('do?', 8753),\n",
       " ('life?', 7753),\n",
       " ('you?', 6295)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_of_embed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "592b6645ef7343af2a8631a6f7ce0b718bb1cb1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%()*+,.:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "p = string.punctuation\n",
    "l = []\n",
    "for y in p:\n",
    "    if y not in [\"'\",\"-\",\"/\",\"&\"]:\n",
    "        l.append(y)\n",
    "        \n",
    "x_ = ''.join([x for x in l])\n",
    "print(x_)\n",
    "def clean_punctuation(token):\n",
    "    '''Cleans text by adjusting punctuations'''   \n",
    "    x = str(token)\n",
    "    for punct in \"-/\":\n",
    "        x = x.replace(punct,' ')\n",
    "    for punct in '&':\n",
    "        # & is present in the embedding hence adding spaces around it.\n",
    "        x = x.replace(punct,f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct,'')\n",
    "    \n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "84c2ff64a1c9a2e60d91a7d7b64cb1470f4ce5d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 44.37% of vocab\n",
      "Found embeddings for 89.26% of all text\n"
     ]
    }
   ],
   "source": [
    "train_data['question_text'] = train_data['question_text'].apply(lambda x:clean_punctuation(x))\n",
    "\n",
    "sentences = train_data['question_text'].apply(lambda x: x.split()).values\n",
    "#print(sentences)\n",
    "vocab = make_vocab(sentences)\n",
    "out_of_embed = present_in_word2vec(vocab,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "33fa2978ced99933ac1c95eda91afd6be053e627"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_numbers(x):\n",
    "\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "15040eacf41134f1acea3dec090b668339fb3afc"
   },
   "outputs": [],
   "source": [
    "train_data[\"question_text\"] = train_data[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "sentences = train_data[\"question_text\"].apply(lambda x: x.split())\n",
    "vocab = make_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "677ff728858d1375d07d7efb8ce128020a7ec446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 46.61% of vocab\n",
      "Found embeddings for 90.02% of all text\n"
     ]
    }
   ],
   "source": [
    "out_of_embed = present_in_word2vec(vocab,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "e2b30d2001e0491aa0981d226cc9fadcabb51603"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "95a053eb5be29b1d96543e82b667348862a05091"
   },
   "outputs": [],
   "source": [
    "# Preparing test data\n",
    "test_data = pd.read_csv('../input/test.csv')\n",
    "test_data['question_text'] = test_data['question_text'].fillna(\"_na_\").values\n",
    "test_data['question_text'] = test_data['question_text'].apply(lambda x:clean_punctuation(x))\n",
    "test_data['question_text'] = test_data['question_text'].apply(lambda x:clean_numbers(x))\n",
    "test_sent = test_data['question_text'].apply(lambda x:x.split())\n",
    "test_vocab = make_vocab(test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "fa2173e32126e91be7b2deb9500db75863f124cd"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set,val_set = train_test_split(train_data,test_size=0.2)\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80\n",
    "\n",
    "train_X = train_set['question_text'].fillna(\"_na_\").values\n",
    "val_X = val_set['question_text'].fillna(\"_na_\").values\n",
    "test_X = test_data['question_text']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "val_X = tokenizer.texts_to_sequences(val_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "train_y = train_set['target'].values\n",
    "val_y = val_set['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "41e7abd01711663b984732a3eb1ba2dfc46d4616",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenizer.word_index.items()\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "cdf5b3107c09fd4135cd6e5f200125be74726841"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "embedding_size = 300\n",
    "for word in embeddings.wv.vocab:\n",
    "    embeddings_index[word] = embeddings.word_vec(word)\n",
    "\n",
    "all_embeddings = np.stack(list(embeddings_index.values()))\n",
    "embed_mean,embed_std = all_embeddings.mean(),all_embeddings.std()\n",
    "num_words = len(tokenizer.word_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "eb35e339878e72d08a373546d9cfd4a7988e7287"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.random.normal(embed_mean,embed_std,(num_words,embedding_size))\n",
    "\n",
    "for word,index in tokenizer.word_index.items():\n",
    "    index -= 1\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "del(embeddings_index)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "7f6ed51d9dc8dd02cc3cd9efef0b92c884325fed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/2\n",
      "1044897/1044897 [==============================] - 1267s 1ms/step - loss: 0.1613 - acc: 0.9448 - val_loss: 0.1403 - val_acc: 0.9492\n",
      "Epoch 2/2\n",
      "1044897/1044897 [==============================] - 1263s 1ms/step - loss: 0.1442 - acc: 0.9495 - val_loss: 0.1344 - val_acc: 0.9509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f07c6b9d550>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dense,Input,Conv2D,Embedding,Dropout,Activation,MaxPool2D,Concatenate,Flatten,Reshape\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "inputs = Input(shape=(80,))\n",
    "word2vec_embedding = Embedding(len(tokenizer.word_index),300,weights = [embedding_matrix],trainable=False)(inputs)\n",
    "reshape = Reshape((80,300,1))(word2vec_embedding)\n",
    "\n",
    "conv_window_3 = Conv2D(512,kernel_size=(3,300),padding='valid',activation='relu')(reshape)\n",
    "conv_window_4 = Conv2D(512,kernel_size=(4,300),padding='valid',activation='relu')(reshape)\n",
    "conv_window_5 = Conv2D(512,kernel_size=(5,300),padding='valid',activation='relu')(reshape)\n",
    "\n",
    "max_pool_window_3 = MaxPool2D(pool_size=(80-3+1,1),strides=(1,1),padding='valid')(conv_window_3)\n",
    "max_pool_window_4 = MaxPool2D(pool_size=(80-4+1,1),strides=(1,1),padding='valid')(conv_window_4)\n",
    "max_pool_window_5 = MaxPool2D(pool_size=(80-5+1,1),strides=(1,1),padding='valid')(conv_window_5)\n",
    "\n",
    "concat_feature_map = Concatenate(axis=1)([max_pool_window_3,max_pool_window_4,max_pool_window_5])\n",
    "flatten = Flatten()(concat_feature_map)\n",
    "dropout = Dropout(0.5)(flatten)\n",
    "\n",
    "output = Dense(units=1,activation='sigmoid')(dropout)\n",
    "\n",
    "model = Model(inputs=inputs,outputs=output)\n",
    "\n",
    "adam = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=adam,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_X,train_y, batch_size=30, epochs=2, verbose=1, validation_data=(val_X, val_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "1eb525dd6906de56e6e50fae0889ae1586e07bbc"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('QIQC_model.hd5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "10457efbc47b029553dedeb2866cddd6b8e5d568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 80, 300)      53436900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 80, 300, 1)   0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 78, 1, 512)   461312      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 77, 1, 512)   614912      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 76, 1, 512)   768512      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3, 1, 512)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1536)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            1537        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 55,283,173\n",
      "Trainable params: 1,846,273\n",
      "Non-trainable params: 53,436,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "464a4b4ac88c022a9761594edb5b0e1807d05832"
   },
   "outputs": [],
   "source": [
    "pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "20d6a39a3c2457651478adac2a3cdf0c82ff84e0"
   },
   "outputs": [],
   "source": [
    "#test_data.head()\n",
    "sub = test_data[['qid']]\n",
    "prediction = np.round(np.argmax(pred,axis=1)).astype(int)\n",
    "sub['prediction'] = prediction\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40a9cbbba337960108a21dab46a901dfd15af999"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
